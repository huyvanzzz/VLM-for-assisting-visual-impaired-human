import tarfile
from torch.utils.data import Dataset
from PIL import Image
import io
from typing import List, Dict
import torch
import numpy as np
from sklearn.model_selection import train_test_split

from .preprocessing import POLMData, construct_prompt, map_metadata_to_ground_truth


class WADDataset(Dataset):
    def __init__(
        self,
        metadata_dataset,
        frame_index: dict,
        bbox_by_folder: dict,
        processor,
        tokenizer,
        split: str = 'train',
        num_frames: int = 1,
        image_size: tuple = (384, 384)
    ):
        self.metadata = metadata_dataset[split]
        self.frame_index = frame_index
        self.bbox_by_folder = bbox_by_folder
        self.processor = processor
        self.tokenizer = tokenizer
        self.split = split
        self.num_frames = num_frames
        self.image_size = image_size
        
        # C·∫•u h√¨nh Tokenizer ƒë·ªÉ ti·∫øt ki·ªám token
        self.tokenizer.padding_side = "right" # Quan tr·ªçng cho training
        self.tokenizer.truncation_side = "right" # Quan tr·ªçng cho training
    def __len__(self):
        return len(self.metadata)

    def _load_frames(self, frame_path: str, frame_ids: List[int]) -> List[Image.Image]:
        """Load v√† x·ª≠ l√Ω ·∫£nh (Padding lu√¥n t·∫°i ƒë√¢y)"""
        # (Gi·ªØ nguy√™n logic load tarfile c·ªßa b·∫°n ƒë·ªÉ code g·ªçn, ch·ªâ th√™m ƒëo·∫°n x·ª≠ l√Ω ·∫£nh)
        shard_to_frames = {}
        for frame_id in frame_ids:
            if frame_id not in self.frame_index[frame_path]:
                raise ValueError(f"Frame {frame_id} not in index")
            frame_info = self.frame_index[frame_path][frame_id]
            shard_path = frame_info['shard']
            if shard_path not in shard_to_frames:
                shard_to_frames[shard_path] = []
            shard_to_frames[shard_path].append((frame_id, frame_info['tar_path']))
        
        frames_dict = {}
        for shard_path, frame_list in shard_to_frames.items():
            with tarfile.open(shard_path, 'r') as tar:
                for frame_id, tar_path in frame_list:
                    member = tar.getmember(tar_path)
                    file_obj = tar.extractfile(member)
                    img = Image.open(io.BytesIO(file_obj.read())).convert('RGB')
                    # -------------------------------------------------
                    frames_dict[frame_id] = img
        
        return [frames_dict[fid] for fid in frame_ids]

    def _load_bboxes(self, frame_path: str, frame_ids: List[int]) -> List[POLMData]:
        # (Gi·ªØ nguy√™n logic c≈© c·ªßa b·∫°n)
        polm_list = []
        if frame_path not in self.bbox_by_folder:
            return polm_list
        for frame_id in frame_ids:
            if frame_id in self.bbox_by_folder[frame_path]:
                bboxes = self.bbox_by_folder[frame_path][frame_id]
                for bbox in bboxes:
                    polm = POLMData(
                        object_type=bbox['label'],
                        bbox=bbox['bbox'],
                        confidence=bbox['confidence']
                    )
                    polm_list.append(polm)
        return polm_list

    def _select_frames_safe(self, frame_path: str, num_frames: int = 1) -> List[int]:
        # (Gi·ªØ nguy√™n logic c≈© c·ªßa b·∫°n)
        if frame_path not in self.frame_index:
            raise ValueError(f"Frame path not in index: {frame_path}")
        available_frames = sorted(self.frame_index[frame_path].keys())
        if len(available_frames) == 0:
            raise ValueError(f"No frames in {frame_path}")
        if num_frames == 1:
            return [available_frames[-1]]
        if len(available_frames) >= num_frames:
            indices = np.linspace(0, len(available_frames) - 1, num_frames, dtype=int)
            return [available_frames[i] for i in indices]
        selected = available_frames.copy()
        while len(selected) < num_frames:
            selected.append(available_frames[-1])
        return selected[:num_frames]

    def __getitem__(self, idx):
        sample = self.metadata[idx]
        frame_path = sample['frame_path']
        
        # 1. Load Data
        frame_ids = self._select_frames_safe(frame_path, num_frames=self.num_frames)
        frames = self._load_frames(frame_path, frame_ids)
        polm_list = self._load_bboxes(frame_path, frame_ids)
        
        # 2. T·∫°o Text
        prompt_text = construct_prompt(polm_list, num_images=self.num_frames)
        ground_truth_dict = map_metadata_to_ground_truth(sample)
        
        # T·ªëi ∆∞u Token: Ch·ªâ l·∫•y JSON string g·ªçn nh·∫•t + Token k·∫øt th√∫c
        answer_text = ground_truth_dict.to_json() + self.tokenizer.eos_token

        # 3. X·ª≠ l√Ω Prompt + Image qua Processor
        # L∆∞u √Ω: padding=False ƒë·ªÉ t·ª± x·ª≠ l√Ω gh√©p chu·ªói th·ªß c√¥ng cho ch√≠nh x√°c
        inputs = self.processor(
            text=prompt_text,
            images=frames,
            return_tensors="pt",
            truncation=False,  # <--- TH√äM D√íNG N√ÄY (B·∫Øt bu·ªôc): C·∫•m processor t·ª± c·∫Øt
            padding=False      # <--- TH√äM D√íNG N√ÄY: ƒê·ªÉ m√¨nh t·ª± x·ª≠ l√Ω padding sau
        )
        
        debug_pixel_values = inputs['pixel_values']
        
        # # In ra m√†n h√¨nh console (GI·ªÆ NGUY√äN C·ª¶A B·∫†N)
        # print(f"\n[DEBUG IMAGE INFO]")
        # print(f" - Shape g·ªëc: {debug_pixel_values.shape}")
        # # Shape th∆∞·ªùng l√†: (Batch, Num_Crops, Channels, Height, Width)
        # # V√≠ d·ª•: torch.Size([1, 3, 3, 384, 384])
        
        # if len(debug_pixel_values.shape) == 5:
        #     n_crops = debug_pixel_values.shape[1]
        #     h = debug_pixel_values.shape[3]
        #     w = debug_pixel_values.shape[4]
        #     print(f" - S·ªë l∆∞·ª£ng m·∫£nh (Crops): {n_crops}")
        #     print(f" - K√≠ch th∆∞·ªõc m·ªói m·∫£nh: {h} x {w}")
        # else:
        #     print(f" - Shape l·∫°: {debug_pixel_values.shape}")
            
        # L·∫•y c√°c Tensor ra kh·ªèi batch dimension (v√¨ processor tr·∫£ v·ªÅ batch=1)
        prompt_input_ids = inputs['input_ids'].squeeze(0)
        prompt_attention_mask = inputs['attention_mask'].squeeze(0)
        pixel_values = inputs['pixel_values'].squeeze(0)

        # ==========================================================================
        # [FIX L·ªñI] CH√àN CODE S·ª¨A L·ªñI T·∫†I ƒê√ÇY (Tr∆∞·ªõc khi gh√©p chu·ªói)
        # ==========================================================================
        image_token_id = self.processor.tokenizer.convert_tokens_to_ids("<image>")
        num_text_tokens = (prompt_input_ids == image_token_id).sum().item()
        
        # 2. ƒê·∫øm s·ªë ng∆∞·ªùi (S·ªë m·∫£nh ·∫£nh th·ª±c t·∫ø model nh√¨n th·∫•y)
        # pixel_values shape th∆∞·ªùng l√†: (S·ªë_l∆∞·ª£ng_m·∫£nh, Channels, H, W)
        num_vision_patches = pixel_values.shape[0] 
        
        # 3. T√≠nh ƒë·ªô l·ªách
        diff = num_vision_patches - num_text_tokens
        
        # 4. In ra b·∫±ng ch·ª©ng (Ch·ªâ in khi c√≥ l·ªách ƒë·ªÉ ƒë·ª° r√°c m√†n h√¨nh)
        if diff != 0:
            print(f"\n==========================================")
            print(f"üïµÔ∏è [CHECKER] PH√ÅT HI·ªÜN L·ªÜCH TOKEN T·∫†I INDEX {idx}")
            print(f"   - Text Prompt c√≥:   {num_text_tokens} th·∫ª <image>")
            print(f"   - Pixel Values c√≥:  {num_vision_patches} m·∫£nh ·∫£nh (patches)")
            print(f"   - ƒê·ªô l·ªách (Diff):   {diff}")
            
            if diff == 1:
                print(f"   -> K·∫æT LU·∫¨N: Th·ª´a ƒë√∫ng 1 m·∫£nh. ƒê√¢y ch√≠nh l√† GLOBAL VIEW (ho·∫∑c CLS context).")
                print(f"   -> K√≠ch th∆∞·ªõc m·∫£nh th·ª´a (c√°i cu·ªëi c√πng): {pixel_values[-1].shape}")
            else:
                print(f"   -> K·∫æT LU·∫¨N: L·ªách {diff} (C√≥ th·ªÉ do nhi·ªÅu frame ho·∫∑c l·ªói logic kh√°c).")
            print(f"==========================================\n")
        
        print(f"\n[DEBUG ALIGNMENT CHECK - AUTO-FIX]")
        print("pixel_values shape:", pixel_values.shape)
        print("pixel_values:", pixel_values.shape[0])
        # N·∫øu l√† 3 m·∫£nh (Features=2052) m√† Token ch·ªâ c√≥ 2051 -> B√π 1 token
        if len(pixel_values.shape) == 4:
            # print(" -> [AUTO-FIX] Ph√°t hi·ªán 2051 tokens (thi·∫øu 1). ƒêang b√π th√™m 1 token <image>...")
            extra_token = torch.tensor([image_token_id], dtype=torch.long)
            extra_mask = torch.tensor([1], dtype=torch.long)
            
            # N·ªëi v√†o ƒëu√¥i Prompt
            prompt_input_ids = torch.cat([prompt_input_ids, extra_token], dim=0)
            prompt_attention_mask = torch.cat([prompt_attention_mask, extra_mask], dim=0)
        # ==========================================================================

        # 4. Tokenize Answer (C√¢u tr·∫£ l·ªùi)
        answer_tokens = self.tokenizer(
            answer_text,
            return_tensors="pt",
            add_special_tokens=False, # Kh√¥ng th√™m BOS n·ªØa v√¨ ƒë√£ c√≥ ·ªü Prompt r·ªìi
            truncation=True,
            max_length=256 # Gi·ªõi h·∫°n ƒë·ªô d√†i c√¢u tr·∫£ l·ªùi ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ
        )
        answer_input_ids = answer_tokens['input_ids'].squeeze(0)
        
        # 5. GH√âP CHU·ªñI (CONCATENATE) -> Logic Training Chu·∫©n
        input_ids = torch.cat([prompt_input_ids, answer_input_ids], dim=0)
        
        # T·∫°o Attention Mask (1 cho c·∫£ prompt v√† answer)
        attention_mask = torch.cat([
            prompt_attention_mask,
            torch.ones_like(answer_input_ids)
        ], dim=0)
        
        # T·∫°o Labels
        # - V√πng Prompt: -100 (Model kh√¥ng c·∫ßn h·ªçc l·∫°i c√¢u h·ªèi)
        # - V√πng Answer: ID c·ªßa token (Model ph·∫£i ƒëo√°n c√¢u tr·∫£ l·ªùi)
        labels = torch.cat([
            torch.full((len(prompt_input_ids),), -100, dtype=torch.long),
            answer_input_ids
        ], dim=0)

        # 7. ƒê√≥ng g√≥i k·∫øt qu·∫£
        return_dict = {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'pixel_values': pixel_values,
            'labels': labels
        }
        
        # (GI·ªÆ NGUY√äN CODE IN C·ª¶A B·∫†N ƒê·ªÇ CHECK L·∫†I)
        image_token_id = self.tokenizer.convert_tokens_to_ids("<image>")

        num_image_tokens = (input_ids == image_token_id).sum().item()

        debug_count = getattr(self, '_debug_count', 0)  # L·∫•y gi√° tr·ªã ho·∫∑c 0

        # if debug_count < 2:
        #     print("\n[DEBUG ALIGNMENT CHECK]")
        #     print("Total input_ids length:", len(input_ids))
        #     print("Image token id:", image_token_id)
        #     print("Number of <image> tokens in text:", num_image_tokens) 
        #     print("Pixel_values shape:", pixel_values.shape)
            
        #     self._debug_count = debug_count + 1  # L∆∞u l·∫°i

        # Copy c√°c th√¥ng tin ph·ª• (quan tr·ªçng cho model Qwen/LLaVA)
        if 'image_sizes' in inputs:
            return_dict['image_sizes'] = inputs['image_sizes'].squeeze(0)
        if 'image_grid_thw' in inputs:
            return_dict['image_grid_thw'] = inputs['image_grid_thw'].squeeze(0)
            
        return return_dict


def build_dataset(config: Dict, processor, tokenizer):
    """Build train/eval datasets from config"""
    
    from datasets import load_dataset
    from collections import defaultdict
    import pickle
    import os
    
    # Load metadata
    print("Loading metadata...")
    metadata = load_dataset(
        config['data']['name'],
        data_files={
            "train": "train.json",
            "test": "test_alter.json"
        }
    )
    
    # Load bboxes
    print("Loading bboxes...")
    bbox_dataset = load_dataset(
        config['data']['name'],
        data_files="all_bboxes.jsonl",
        split="train"
    )
    
    bbox_by_folder = defaultdict(lambda: defaultdict(list))
    for bbox_entry in bbox_dataset:
        folder_id = bbox_entry['folder_id']
        frame_id = bbox_entry['frame_id']
        
        bbox_by_folder[folder_id][frame_id].append({
            'label': bbox_entry['label'],
            'confidence': bbox_entry['probs'],
            'bbox': bbox_entry['boxs']
        })
    
    # Load frame index
    print("Loading frame index...")
    index_file = "./wad_dataset/frame_index.pkl"
    
    if os.path.exists(index_file):
        with open(index_file, 'rb') as f:
            frame_index = pickle.load(f)
    else:
        raise FileNotFoundError(f"Frame index not found at {index_file}. Run build_frame_index.py first.")
    
    # Create datasets
    train_dataset = WADDataset(
        metadata_dataset=metadata,
        frame_index=frame_index,
        bbox_by_folder=bbox_by_folder,
        processor=processor,
        tokenizer=tokenizer,
        split='train',
        num_frames=config['data']['num_frames'],
        image_size=tuple(config['model']['vision']['image_size'])
    )
    
    # Train/val split
    train_size = config['data']['train_split']
    indices = list(range(len(train_dataset)))
    
    train_indices, val_indices = train_test_split(
        indices,
        train_size=train_size,
        random_state=config['data']['seed']
    )
    
    from torch.utils.data import Subset
    train_subset = Subset(train_dataset, train_indices)
    val_subset = Subset(train_dataset, val_indices)
    
    print(f"‚úì Train: {len(train_subset)}, Val: {len(val_subset)}")
    
    return train_subset, val_subset