{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation & Analysis\n",
    "## Đánh giá chi tiết các VLM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from src.models.model_registry import build_model\n",
    "from src.data.wad_dataset import build_dataset\n",
    "from src.evaluation.evaluator import VLMEvaluator\n",
    "from src.utils.visualization import plot_model_comparison\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "config_path = '../configs/llava_config.yaml'\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Config loaded:\")\n",
    "print(f\"  Model: {config['model']['name']}\")\n",
    "print(f\"  Output dir: {config['training']['output_dir']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "print(\"Loading model...\")\n",
    "vlm = build_model(config)\n",
    "\n",
    "# Load checkpoint (if exists)\n",
    "checkpoint_path = Path(config['training']['output_dir']) / 'final_model'\n",
    "\n",
    "if checkpoint_path.exists():\n",
    "    from peft import PeftModel\n",
    "    vlm.model = PeftModel.from_pretrained(vlm.model, str(checkpoint_path))\n",
    "    print(f\"✓ Loaded checkpoint from {checkpoint_path}\")\n",
    "else:\n",
    "    print(\" No checkpoint found, using base model\")\n",
    "\n",
    "vlm.model.eval()\n",
    "print(\"✓ Model ready for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset\n",
    "print(\"Loading evaluation dataset...\")\n",
    "train_dataset, eval_dataset = build_dataset(config, vlm.processor, vlm.tokenizer)\n",
    "\n",
    "print(f\"✓ Evaluation dataset: {len(eval_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluator\n",
    "evaluator = VLMEvaluator(\n",
    "    model=vlm.model,\n",
    "    tokenizer=vlm.tokenizer,\n",
    "    processor=vlm.processor,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Run evaluation (takes time!)\n",
    "results = evaluator.evaluate(eval_dataset)\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for metric, score in results.items():\n",
    "    print(f\"  {metric}: {score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics\n",
    "metrics_df = pd.DataFrame([results]).T\n",
    "metrics_df.columns = ['Score (%)']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "metrics_df.plot(kind='barh', legend=False)\n",
    "plt.xlabel('Score (%)', fontsize=12)\n",
    "plt.ylabel('Metric', fontsize=12)\n",
    "plt.title('Model Evaluation Metrics', fontsize=14, fontweight='bold')\n",
    "plt.xlim(0, 100)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../experiments/results/evaluation_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Qualitative Analysis (Sample Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for first 5 samples\n",
    "num_samples = 5\n",
    "\n",
    "print(\"Generating sample predictions...\\n\")\n",
    "\n",
    "for idx in range(num_samples):\n",
    "    sample = eval_dataset[idx]\n",
    "    \n",
    "    # Prepare input\n",
    "    inputs = {\n",
    "        'input_ids': sample['input_ids'].unsqueeze(0).to(config['hardware']['device']),\n",
    "        'attention_mask': sample['attention_mask'].unsqueeze(0).to(config['hardware']['device']),\n",
    "        'pixel_values': sample['pixel_values'].unsqueeze(0).to(config['hardware']['device'])\n",
    "    }\n",
    "    \n",
    "    if 'image_sizes' in sample:\n",
    "        inputs['image_sizes'] = [tuple(sample['image_sizes'].tolist())]\n",
    "    \n",
    "    if 'image_grid_thw' in sample:\n",
    "        grid = sample['image_grid_thw']\n",
    "        if grid.dim() == 1:\n",
    "            grid = grid.unsqueeze(0)\n",
    "        inputs['image_grid_thw'] = grid.unsqueeze(0).to(config['hardware']['device'])\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = vlm.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    pred_text = vlm.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract ground truth\n",
    "    labels = sample['labels']\n",
    "    gt_tokens = labels[labels != -100]\n",
    "    gt_text = vlm.tokenizer.decode(gt_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Sample {idx+1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Ground Truth:\\n{gt_text}\\n\")\n",
    "    print(f\"Prediction:\\n{pred_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Multiple Models (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load comparison results (if exists)\n",
    "comparison_file = '../experiments/comparison.json'\n",
    "\n",
    "if Path(comparison_file).exists():\n",
    "    with open(comparison_file, 'r') as f:\n",
    "        all_results = json.load(f)\n",
    "    \n",
    "    # Plot comparison\n",
    "    plot_model_comparison(all_results, '../experiments/results/model_comparison.png')\n",
    "    \n",
    "    print(\"\\nModel Comparison:\")\n",
    "    df = pd.DataFrame(all_results).T\n",
    "    print(df.to_string())\n",
    "else:\n",
    "    print(\" No comparison results found. Run: python run_experiments.py --configs configs/*.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors by field\n",
    "field_accuracies = {k: v for k, v in results.items() if 'accuracy' in k}\n",
    "\n",
    "sorted_fields = sorted(field_accuracies.items(), key=lambda x: x[1])\n",
    "\n",
    "print(\"Field Accuracy (lowest to highest):\")\n",
    "for field, acc in sorted_fields:\n",
    "    print(f\"  {field}: {acc:.2f}%\")\n",
    "    \n",
    "print(\"\\n Focus improvement on lowest accuracy fields!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
