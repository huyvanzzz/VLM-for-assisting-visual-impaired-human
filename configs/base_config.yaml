# ===== EXPERIMENT METADATA =====
experiment:
  name: "wad_vlm_training"
  version: "1.0"
  description: "Vision-Language Model for Autonomous Driving"
  tags: ["vlm", "navigation", "blind-assistance"]

# ===== MODEL SELECTION =====
model:
  architecture: "llava"  # Options: llava, qwen, blip2, instructblip
  name: "llava-hf/llava-onevision-qwen2-0.5b-ov-hf"
  
  # LoRA config
  lora:
    enabled: true
    r: 16
    alpha: 16
    dropout: 0.05
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
  
  # Quantization
  quantization:
    enabled: true
    bits: 4
    type: "nf4"
    double_quant: true
  
  # Vision config
  vision:
    freeze_encoder: true
    max_tiles: 1

# ===== DATASET =====
data:
  name: "minhdang0901/WAD_Images"
  num_frames: 1
  train_split: 0.9
  seed: 42
  max_samples: null  # null = use all

# ===== TRAINING =====
training:
  output_dir: "./outputs"
  num_epochs: 3
  batch_size: 1
  gradient_accumulation_steps: 2
  learning_rate: 2e-4
  warmup_steps: 100
  weight_decay: 0.01
  fp16: true
  gradient_checkpointing: true
  
  # Optimizer
  optimizer: "paged_adamw_8bit"
  lr_scheduler: "cosine"
  
  # Logging
  logging_steps: 5
  eval_steps: 500
  save_steps: 500
  save_total_limit: 2
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 3
    metric: "eval_loss"

# ===== EVALUATION =====
evaluation:
  metrics: ["bleu", "rouge", "exact_match", "f1"]
  batch_size: 2

# ===== HARDWARE =====
hardware:
  device: "cuda"
  num_workers: 0
  pin_memory: false
  cuda_alloc_conf: "expandable_segments:True,max_split_size_mb:128"

# ===== EXPERIMENT TRACKING =====
tracking:
  enabled: true
  backend: "mlflow"  # Options: mlflow, wandb, tensorboard
  project_name: "wad-vlm"
  log_model: true