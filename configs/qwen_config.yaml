# ===== QWEN-VL CONFIG =====
experiment:
  name: "wad_qwen_vl_training"
  version: "1.0"
  description: "Qwen-VL for Autonomous Driving"
  tags: ["qwen-vl", "navigation"]

model:
  architecture: "qwen"
  name: "Qwen/Qwen2-VL-2B-Instruct"
  
  lora:
  enabled: true
  r: 16                    # OK
  alpha: 16               # Thường dùng alpha = 32 (2*r)
  dropout: 0.05           # OK
  target_modules: 
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    # Tùy chọn thêm:
    # - "gate_proj"
    # - "up_proj"
    # - "down_proj"
  bias: "none"
  task_type: "CAUSAL_LM"
  
  quantization:
    enabled: true
    bits: 4
    type: "nf4"
    double_quant: true
    compute_dtype: "bfloat16"
  
  vision:
    freeze_encoder: true
    min_pixels: 256 * 28 * 28
    max_pixels: 1280 * 28 * 28

data:
  name: "minhdang0901/WAD_Images"
  num_frames: 1
  train_split: 0.9
  seed: 42

training:
  output_dir: "./outputs/qwen_vl"
  num_epochs: 3
  batch_size: 1
  gradient_accumulation_steps: 1
  learning_rate: 2e-4
  warmup_ratio: 0.03
  weight_decay: 0.01
  bf16: true
  fp16: false
  gradient_checkpointing: true
  optimizer: "adamw_bnb_8bit"
  lr_scheduler: "cosine"
  max_grad_norm: 1.0
  logging_steps: 5
  eval_steps: 500
  save_steps: 500
  save_total_limit: 2

evaluation:
  metrics: ["exact_match", "field_accuracy"]
  batch_size: 2

hardware:
  device: "cuda"
  num_workers: 0
  pin_memory: false
  cuda_alloc_conf: "expandable_segments:True"

tracking:
  enabled: true
  backend: "mlflow"
  project_name: "wad-vlm"
  log_model: true